{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-14T18:52:51.846571100Z",
     "start_time": "2023-11-14T18:52:43.232895200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.io import read_video\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from note_detector.python.video_note_detector import generate_labels\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[[ 95,  92, 107],\n          [ 95,  92, 107],\n          [ 95,  92, 107],\n          ...,\n          [ 82,  84,  98],\n          [ 82,  84,  98],\n          [ 82,  84,  98]],\n \n         [[ 95,  92, 107],\n          [ 95,  92, 107],\n          [ 95,  92, 107],\n          ...,\n          [ 82,  84,  98],\n          [ 82,  84,  98],\n          [ 82,  84,  98]],\n \n         [[ 95,  92, 107],\n          [ 95,  92, 107],\n          [ 95,  92, 107],\n          ...,\n          [ 82,  84,  98],\n          [ 82,  84,  98],\n          [ 82,  84,  98]],\n \n         ...,\n \n         [[ 89,  91, 105],\n          [ 89,  91, 105],\n          [ 89,  91, 105],\n          ...,\n          [ 81,  81,  93],\n          [ 81,  81,  93],\n          [ 81,  81,  93]],\n \n         [[ 89,  91, 105],\n          [ 89,  91, 105],\n          [ 89,  91, 105],\n          ...,\n          [ 81,  81,  93],\n          [ 81,  81,  93],\n          [ 81,  81,  93]],\n \n         [[ 89,  91, 105],\n          [ 89,  91, 105],\n          [ 89,  91, 105],\n          ...,\n          [ 81,  81,  93],\n          [ 81,  81,  93],\n          [ 81,  81,  93]]], dtype=torch.uint8),\n ['D_4', 'D_5'])"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# have > 1 video and need dataset to be a LINEARLY sampled set of images from each video, labeled with the note being played in that frame\n",
    "# we do know the length of the dataset given the number of videos bc it is (# videos) * (# samples taken per video [SPV])\n",
    "# in __init__: \n",
    "#   - Run the API conversion of all the videos to their labeled notes + which frame the note corresponds to - store these in an array; spot idx is the note for sampled image number idx\n",
    "#   - Extract all the frames from the videos and store them in an array; spot idx of the array is frame [idx % SPV] of video [floor of idx / SPV]\n",
    "# in __getitem__(idx), return the values (image, label) at spot idx of the 2 arrays created in __init__\n",
    "\n",
    "class NoteDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.frame_labels = []\n",
    "        self.frames = []\n",
    "        \n",
    "        # loop over each training video to assign a label to each frame and aggregate them all in one training array(s)\n",
    "        for file in os.listdir(\"./training_data\"):\n",
    "            \n",
    "            # get frames of video and store them in self.frames\n",
    "            v_frames, _, _ = read_video(\"./training_data/%s\" % file)\n",
    "            for frame_num in range(v_frames.shape[0]):\n",
    "                self.frames.append(v_frames[frame_num])\n",
    "\n",
    "            \n",
    "            # use library to get labels for each frame\n",
    "            cur_video_labels, num_frames = generate_labels(\"./training_data/\", file)\n",
    "            tmp_label_aggregator = [[] for i in range(num_frames)]\n",
    "            for frame, note in cur_video_labels:\n",
    "                tmp_label_aggregator[int(frame)].append(note)\n",
    "            self.frame_labels.extend(tmp_label_aggregator)\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.frame_labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.frames[idx], self.frame_labels[idx]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T18:52:55.914478600Z",
     "start_time": "2023-11-14T18:52:51.835176100Z"
    }
   },
   "id": "a87e86cabc7595c9"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# video_frames, _, _ = read_video(\"./training_data/%s\" % \"dumb_scale_youtube.mp4\")\n",
    "# video_frames[0].shape\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T18:52:55.915506800Z",
     "start_time": "2023-11-14T18:52:55.911390500Z"
    }
   },
   "id": "ac0be9eacbd40713"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# cur_video_labels, num_frames = generate_labels(\"./training_data/\", \"dumb_scale_youtube.mp4\")\n",
    "# print(num_frames)\n",
    "# for frame, note in cur_video_labels:\n",
    "#     print(frame, \" ; \", note)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T18:52:55.943191800Z",
     "start_time": "2023-11-14T18:52:55.917414700Z"
    }
   },
   "id": "ad01e971618c9d73"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
