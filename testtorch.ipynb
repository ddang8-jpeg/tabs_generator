{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-26T21:01:39.027880500Z",
     "start_time": "2023-11-26T21:01:39.012706700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.io import VideoReader\n",
    "from torchvision.io import write_jpeg, read_image\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import os\n",
    "from note_detector.python.video_note_detector import generate_labels\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Function to convert videos to valid training data that we can use, in the structure shown in ./training_data.\n",
    "# The ith element of the array in \"training_data/labels.npy\" is the label(s) for the image named \"i.jpg\" in \"training_data/frames\"\n",
    "\n",
    "def convert_video_to_training_data(vid_path, vid_name, resize_h, resize_w):\n",
    "    start_num = 0\n",
    "    cur_labels = []\n",
    "    if os.path.isfile(\"training_data/labels.npy\"):\n",
    "        cur_labels = np.load(\"training_data/labels.npy\", allow_pickle=True).tolist()\n",
    "        start_num = len(cur_labels)\n",
    "    \n",
    "    print(\"processing file\", vid_name)\n",
    "    \n",
    "    # get frames of video and save sequentially in training_data/frames\n",
    "    reader = VideoReader(vid_path + vid_name, \"video\")\n",
    "    for frame in reader:\n",
    "        cur_img = frame[\"data\"]\n",
    "        img_to_save = transforms.Resize((resize_h, resize_w))(cur_img)\n",
    "        img_name = \"./training_data/frames/%s.jpg\" % start_num\n",
    "        start_num += 1\n",
    "        write_jpeg(img_to_save, img_name)\n",
    "\n",
    "    print(\"done w video read\")\n",
    "\n",
    "    # use library to get labels for each frame\n",
    "    cur_video_labels, num_frames = generate_labels(vid_path, vid_name)\n",
    "    tmp_label_aggregator = [[] for i in range(num_frames)]\n",
    "    for frame, note in cur_video_labels:\n",
    "        tmp_label_aggregator[int(frame)].append(note)\n",
    "    cur_labels.extend(tmp_label_aggregator)\n",
    "    np.save(\"training_data/labels.npy\", np.array(cur_labels, dtype=object))\n",
    "\n",
    "    print(\"done w labeling\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-26T20:41:28.121243800Z",
     "start_time": "2023-11-26T20:41:28.104471700Z"
    }
   },
   "id": "700e58f5436d6beb"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing file e5.MOV\n",
      "done w video read\n",
      "done w labeling\n"
     ]
    }
   ],
   "source": [
    "# Call the above function to generate usable training data\n",
    "\n",
    "FRAME_HEIGHT = 1080\n",
    "FRAME_WIDTH = 1920\n",
    "\n",
    "answer = input(\"Are you sure you want to modify the training_data folder? (y/n)\")\n",
    "\n",
    "if answer == \"y\":\n",
    "    convert_video_to_training_data(\"./vids/\", \"dumb_scale_youtube.mp4\", FRAME_HEIGHT, FRAME_WIDTH)\n",
    "    \n",
    "    convert_video_to_training_data(\"./vids/\", \"e1.MOV\", FRAME_HEIGHT, FRAME_WIDTH)\n",
    "    \n",
    "    convert_video_to_training_data(\"./vids/\", \"e2_untrimmed.MOV\", FRAME_HEIGHT, FRAME_WIDTH)\n",
    "    \n",
    "    convert_video_to_training_data(\"./vids/\", \"e3_untrimmed.MOV\", FRAME_HEIGHT, FRAME_WIDTH)\n",
    "    \n",
    "    convert_video_to_training_data(\"./vids/\", \"e5.MOV\", FRAME_HEIGHT, FRAME_WIDTH)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-26T20:51:40.265010800Z",
     "start_time": "2023-11-26T20:51:25.275250300Z"
    }
   },
   "id": "d46e92a5049a716b"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# define the dataset\n",
    "\n",
    "class NoteDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.labels = np.load(\"training_data/labels.npy\", allow_pickle=True).tolist()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return read_image(\"./training_data/frames/%s.jpg\" % idx), self.labels[idx]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-26T21:01:50.992347900Z",
     "start_time": "2023-11-26T21:01:50.961940Z"
    }
   },
   "id": "a87e86cabc7595c9"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# create the dataloader\n",
    "\n",
    "batch_size = 64\n",
    "FIXED_H = 1080\n",
    "FIXED_W = 1920\n",
    "\n",
    "train_dataset = NoteDataset()\n",
    "dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-26T21:01:53.255012100Z",
     "start_time": "2023-11-26T21:01:53.221001900Z"
    }
   },
   "id": "b906dde3bcdd076f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# define the CNN\n",
    "\n",
    "# each input to the network is 3x1080x1920\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.stack = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, )\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6b04ba92e53e500"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "19ef78cae0d37159"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "5f7e83e9a91b4668"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# quick debug commands:\n",
    "\n",
    "# vid_path, vid_name = \"./vids/\", \"dumb_scale_youtube.mp4\"\n",
    "# # cur_video_labels, num_frames = generate_labels(vid_path, vid_name)\n",
    "# reader = VideoReader(vid_path + vid_name, \"video\")\n",
    "# frame = next(reader)[\"data\"]\n",
    "# print(frame.shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "604bf7f6df564996"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# a = np.load(\"training_data/labels.npy\", allow_pickle=True)\n",
    "# print(len(a))\n",
    "# print(a[0])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "45f3089eac92e8a1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# f, l = train_dataset[0]\n",
    "# print(len(train_dataset))\n",
    "# print(type(f))\n",
    "# print(type(l))\n",
    "# print(f.shape)\n",
    "# print(l)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "87f68d69b61b41d2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# processed_X = []\n",
    "# processed_y = []\n",
    "# \n",
    "# tmp_frames = []\n",
    "# mn_H = math.inf\n",
    "# mn_W = math.inf\n",
    "# \n",
    "# # loop over each training video to assign a label to each frame and aggregate them all in one training array(s)\n",
    "# for file in os.listdir(\"./training_data\"):\n",
    "#     print(\"processing file\", file)\n",
    "#     \n",
    "#     # get frames of video and update the dimensions to resize to later\n",
    "#     v_frames, _, _ = read_video(\"./training_data/%s\" % file, output_format=\"TCHW\")\n",
    "#     mn_H = min(mn_H, v_frames.shape[2])\n",
    "#     mn_W = min(mn_W, v_frames.shape[3])\n",
    "#     for frame_num in range(v_frames.shape[0]):\n",
    "#         tmp_frames.append(v_frames[frame_num])\n",
    "#     \n",
    "#     print(\"done w video read\")\n",
    "# \n",
    "#     \n",
    "#     # use library to get labels for each frame\n",
    "#     cur_video_labels, num_frames = generate_labels(\"./training_data/\", file)\n",
    "#     tmp_label_aggregator = [[] for i in range(num_frames)]\n",
    "#     for frame, note in cur_video_labels:\n",
    "#         tmp_label_aggregator[int(frame)].append(note)\n",
    "#     processed_y.extend(tmp_label_aggregator)\n",
    "#     \n",
    "#     print(\"done w labeling\")\n",
    "# \n",
    "# # resize all frames to be the same size so the NN can handle them\n",
    "# for frame in tmp_frames:\n",
    "#     tnsr = transforms.Resize((mn_H, mn_W))(frame)\n",
    "#     processed_X.append(tnsr.numpy())\n",
    "# \n",
    "# processed_X = np.array(processed_X)\n",
    "# processed_y = np.array(processed_y, dtype=object)\n",
    "# \n",
    "# np.save(\"processed_frame_data\", processed_X)\n",
    "# np.save(\"processed_labels\", processed_y)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c7181569b94e32c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
